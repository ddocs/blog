{"archive":{"blogPosts":[{"id":"scalability-trilemma","metadata":{"permalink":"/blog/scalability-trilemma","source":"@site/blog/2026-02-19-scalability-trilemma.md","title":"How the Scalability Trilemma drives blockchain innovation","description":"The Scalability Trilemma (coined by Vitalik Buterin) is a concept worth digging into as advances in blockchain technology truly arise from trying to resolve it. A fascinating array of solutions drives the expansion of chains and projects.","date":"2026-02-19T00:00:00.000Z","tags":[{"inline":true,"label":"cryptocurrency","permalink":"/blog/tags/cryptocurrency"},{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"scalability","permalink":"/blog/tags/scalability"},{"inline":true,"label":"innovation","permalink":"/blog/tags/innovation"}],"readingTime":7.15,"hasTruncateMarker":true,"authors":[{"name":"Anita Diamond","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"scalability-trilemma","title":"How the Scalability Trilemma drives blockchain innovation","authors":{"name":"Anita Diamond"},"tags":["cryptocurrency","blockchain","scalability","innovation"]},"unlisted":false,"nextItem":{"title":"Can we really trust in crypto software?","permalink":"/blog/what-do-we-mean-by-trust"}},"content":"The Scalability Trilemma (coined by Vitalik Buterin) is a concept worth digging into as advances in blockchain technology truly arise from trying to resolve it. A fascinating array of solutions drives the expansion of chains and projects. \n\n{/* truncate */}\n\nEssentially, there are three key properties that a blockchain system needs to balance; but achieving all three attributes is a challenge that eludes most blockchain systems resulting in tradeoffs between these competing attributes. The trilemma posits that a blockchain can only optimize two out of three critical properties.\n\n<img src={require('@site/static/img/scalability-trilemma.png').default} alt='Scalability Trilemma' />\n\nThe three properties are:\n\n**Scalability**: Scalability refers to how well a blockchain can handle a growing number of transactions. \n\n**Decentralization**: Decentralization ensures no single entity controls a blockchain. A decentralized blockchain relies on a global network of computers, called nodes, running the blockchain's software. The chain can run without any trust dependencies on a small group of actors.\n\n**Security**: Security ensures that transactions are valid and immutable once confirmed. The chain can resist a large percentage of participating nodes trying to attack it.\n\n## The Problem\n\nThe pioneer blockchain designs of Bitcoin and Ethereum prioritised decentralization and security leading to troublesome scalability issues as popularity increased. Gas and transaction fees reached formidable heights and led to virtual unusability in 2018 with transactions per second unable to come anywhere close to meeting demand. \n\nThese bottlenecks spurred a wave of research with each approach attempting to hold onto the core tenets of blockchain. \n\n## The Solutions\n\nSolutions range in their success, whilst sharding and some layer 2 protocols have been successful, others such as federated blockchains have not. \n\n### Sharding\n\nThe concept of sharding, originally from database management, involves\ndividing a larger database into numerous smaller datasets across various nodes.\nA blockchain network is divided into multiple shards (*n* number of shards) with each\nshard consisting of a number of nodes (*m*). The total number of nodes in the blockchain network is *n × m*.\n\nEach shard maintains its own independent blockchain, processing its\nown sequence of blocks and transactions without any interactions with other shards (transactions are processed in parallel within their respective shards unless a cross-shard communication mechanism is implemented). \n\n:::info\nCross-shard transactions are more complex because they cause higher latency due to communication between shards, additional coordination overhead to manage transaction steps across shards and security challenges in maintaining consistent and secure data across multiple shards.\n:::\n\nSharding techniques can be **Static**, **Dynamic** or **Layered**. \n\n#### Sharding techniques\n\nStatic Sharding is a technique that determines the shard size and number of shards\nbased on the number of nodes that are then allocated equally across different shards.\n\nDynamic Sharding facilitates dynamic resource sharing to cope with demand. That may allocate the nodes across different shards and dynamically change the shard sizes and number of shards.\n\nLayered Sharding entails allocating a shard with transactions and if it reaches capacity, any subsequent transactions are assigned to a shard with sufficient processing capability.\n\nZilliqa (ZIL) was the first public blockchain to implement sharding. \nNEAR Protocol uses a dynamic sharding approach called \"Nightshade,\" where shards are added or removed based on network traffic, ensuring efficient scaling.\nTON (The Open Network) uses an \"Infinite Sharding Paradigm,\" where shards can automatically split or merge based on transaction load.\n\n**Challenges are:** \n\n1. Scaling Byzantine Fault Tolerance consensus protocols. Scaling needs to work across the three phases of practical Byzantine fault tolerance. Pre-prepare, Prepare and Commit. \n\n2. Ensuring secure and efficient shard formation \n\n3. Enabling secure distributed transactions even with malicious coordinators. \n\nWhilst successful as a solution, sharding is not without downsides. \nWhilst improving scalability, it introduces cross-shard latency and reduces per-shard decentralization. \n\n### Zero-Knowledge Rollups (ZK Rollups)\n\nZero-knowledge rollups or zk-rollups bundle hundreds of transactions off-chain. User transactions are submitted and executed by a Layer 2 operator. A cryptographic proof of their validity is then generated by a 'prover' often a SNARK or STARK. They confirm that the transactions are valid and this mathematical proof is submitted to the Layer 1 chain along with the bundles of transactions as a smart contract. \n\nThe Layer 1 chain uses a smart contract to verify the mathematical proof. If it is successful then the transaction is updated on-chain with near-instant finality. \n\nThis approach dramatically reduces network congestion, lowers gas costs and increases throughput while security is inherited from the underlying Layer 1 chain. \n\n**Challenges are:**\n\n1. Layer-2 protocols like ZK-rollups enhance throughput but rely on centralized provers/ This can potentially create new attack surfaces.\n\n2. Possible trade-off with decentralization due to centralized provers. \n\nIt's mainly Layer 2 scaling solutions for Ethereum that use zk-rollups. Popular ZK-rollup blockchains include StarkNet, zkSync, and Polygon Zero.\n\n### State Channels\n\nState channels enable two or more participants to securely transact off-chain with only the initial opening and final closing states recorded on the main blockchain. \n\nChannels are simple peer-to-peer protocols. They use cryptography to demonstrate that the summary data they generate is the result of a valid set of transactions. A **multisig** smart contract ensures the transactions are signed by the correct parties.\n\nParticipants lock funds in a smart contract, exchange signed updates directly, and settle the final balance on-chain.\n\nThis allows for high-frequency, private, and low-cost transaction throughput and lower costs for users. \n\nThe Lightning Network is probably the most well-known project using state channels for Bitcoin. Ethereum (ETH) Uses the Raiden Network for ERC-20 token transfers and specialized platforms like Connext (formerly part of SpankChain) and Counterfactual for general state updates (e.g., in gaming or dApps).\n\n**Challenges are:**\n\n1. A state channel can fail potentially putting funds at risk. \n\nThe following solutions have been less effective at achieving the aims of balancing the central tenets of the blockchain whilst improving scalability.  \n\n### Delegated Proof-of-Stake (dPoS)\n\nDelegated Proof of Stake (DPoS) is a blockchain consensus mechanism where users vote to elect delegates who validate transactions and create new blocks. It combines a democratic process with a collateral staking system to enhance efficiency and address the limitations of traditional Proof of Stake.\n\nWith Proof-of-Stake, the higher a stake a validator has in the blockchain, the greater likelihood they are selected to validate a transaction. In doing so, they earn the transaction fees associated with the transaction. In delegated-Proof-of-Stake (dPoS), token holders vote for the delegate they choose to validate the transaction. In doing so, they consider factors like reputation and reliability. This additional voting layer prevents centralization whilst also improving transaction speeds. dPoS prevents the same validators being repeatedly chosen to perform validation simply because they have the highest stake regardless of their performance. This democratization of the validation process is thought to make the chain less centralized and so more secure from bad actors. \n\nTransaction speeds increase because the need to wait until a certain number of untrusted nodes have verified a transaction before it can be confirmed is eliminated.\n\nProjects using dPoS include EOS with 21 elected block producers.\nTRON (TRX) with 27 Super Representatives (SRs) for network consensus and\nBitShares - One of the earliest platforms to implement DPoS, focusing on decentralized exchanges.\n\n**Challenges:**\n\n1. dPoS does not necessarily tackle the centralization tendencies of Proof-of-Stake, it simply reintroduces centralization along different lines. For example, a particular validator may be the most reliable and efficient and so is constantly voted for.\n\n2. Trust is not distributed architecturally within the system technologically, it becomes prone to the risk of potential collusion among delegates. Security is reduced.\n\n3. dPoS achieves scalability but at the expense of decentralization. \n\n### Federated Blockchains\n\nFederated or consortium blockchains blend features of private blockchains and public blockchains. They are typically set up with only a few participants such as investment banks or other financial providers. Validators are pre-selected and then all other participating nodes must agree on a transaction for it to be added to the chain. \n\nR3 Corda is a prominent platform utilized by over 60 financial institutions to provide consortium blockchain services.\n\nIBM Food Trust (Supply Chain) connects retailers, suppliers, and producers (e.g. Walmart, Nestle) to enhance transparency and food traceability.\n\n\n**Challenges**\n\n1. This solution optimizes security and scalability for its participants very much at the expense of decentralization. Such purposes provide utility for a different set of usecases than traditional blockchains. \n\n### Conclusion\n\nThe blockchain trilemma endures as a limitation in the evolution of decentralized systems. The pursuit of solutions that harmonize security, scalability, and decentralization remains an important driver of innovative and bleeding edge technology."},{"id":"what-do-we-mean-by-trust","metadata":{"permalink":"/blog/what-do-we-mean-by-trust","source":"@site/blog/2024-09-24-what-do-we-mean-by-trust.md","title":"Can we really trust in crypto software?","description":"What do we mean by Trust and Trustlessness? Are these concepts relevant when systems are created and run by flawed humans.","date":"2024-09-24T00:00:00.000Z","tags":[{"inline":true,"label":"cryptocurrency","permalink":"/blog/tags/cryptocurrency"},{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"trust","permalink":"/blog/tags/trust"}],"readingTime":1.26,"hasTruncateMarker":true,"authors":[{"name":"Anita Diamond","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"what-do-we-mean-by-trust","title":"Can we really trust in crypto software?","authors":{"name":"Anita Diamond"},"tags":["cryptocurrency","blockchain","trust"]},"unlisted":false,"prevItem":{"title":"How the Scalability Trilemma drives blockchain innovation","permalink":"/blog/scalability-trilemma"},"nextItem":{"title":"How this Nobel Prize winning Economist might save the world?","permalink":"/blog/elinor-ostrom"}},"content":"What do we mean by Trust and Trustlessness? Are these concepts relevant when systems are created and run by flawed humans. \n\n<!-- truncate -->\n\n## Mental models \nAlthough software components can be labeled as 'trusted', trustworthiness is not a black-and-white issue. It is meaningless without answers to the questions, “Trusted by whom?” and “Trusted to do what?” Similarly, we cannot define security policies without asking, “Secure from whom?” and “Secure against what?”. This specificity is important. \n\nAttempting to classify all programs as simply trusted or untrusted is not always helpful, yet some security experts continue to think along such lines. The missing component in this common and dangerous oversimplification is the mental model. Simson Garfinkel and Gene Spafford wrote that “a computer is secure if you can depend on it and its software to behave as you expect.”\n\nFulfilling expectations is a matter of keeping behaviour and expectations in agreement, and the users’ expectations are based on their mental model of the system. Both the security policy and mental model are dynamic; they change in response to user actions.\n\nThe lure of decentralised finance or the Defi movement has been enabled by blockchain and distributed ledger technologies that distribute trust evenly across a network. The Blockchain has changed the landscape of economic interactions by allowing people to place trust in abstract concepts rather than institutions or other third parties.\n\nHowever, the current debate on centralisation versus decentralisation often overlooks the complexity of the socio-technical ecology we inhabit, and the significance of human behaviour."},{"id":"elinor-ostrom","metadata":{"permalink":"/blog/elinor-ostrom","source":"@site/blog/2024-03-28-tragedy-of-the-commons.md","title":"How this Nobel Prize winning Economist might save the world?","description":"commons","date":"2024-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"self-governance","permalink":"/blog/tags/self-governance"},{"inline":true,"label":"economics","permalink":"/blog/tags/economics"}],"readingTime":2.27,"hasTruncateMarker":false,"authors":[{"name":"Anita","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"elinor-ostrom","title":"How this Nobel Prize winning Economist might save the world?","authors":{"name":"Anita"},"tags":["self-governance","economics"]},"unlisted":false,"prevItem":{"title":"Can we really trust in crypto software?","permalink":"/blog/what-do-we-mean-by-trust"}},"content":"![commons](https://upload.wikimedia.org/wikipedia/commons/6/63/Medieval_Open_Field_System.JPG)\n\nRecently, I found myself waking up bolt upright and surprisingly alert at 3am. I turned on my laptop to check my email and before I knew it I was learning all about a remarkable female Nobel prize winning economist called [**Elinor Ostrom**](https://en.wikipedia.org/wiki/Elinor_Ostrom). \n\nI'm surprised to have not heard her name more widely mentioned because her research is so hopeful, pertinent and pragmatic to the biggest challenges we are facing today - [**The Climate Crisis**](https://www.greenpeace.org.uk/challenges/climate-change/solutions-climate-change/). \n\nIn a nutshell, Elinor Ostrom found numerous case studies of groups compelled to share natural resources like pastures, fishing waters, grazing land and forests. Her field based research effectively disproved the prevailing assumptions of economists that collectively used natural resources would be over-exploited and destroyed in the long-term - the so called ['**Tragedy of the Commons**'].\n\nWonderfully, Elinor found instead that groups and communities found ways to **share natural resources for the benefit of all** and in a way that **promoted their longevity**. The way these communities were able to do so was by **collaborating and cooperating**. Over time, communities developed self-governing rules that promoted economic and ecological sustainability. \n\n\n<img src=\"https://www.aapss.org/wp-content/uploads/2024/02/elinor-ostrom.png\" width=\"200\" />\n\n\nThe Tragedy of the Commons refers to a situation in which individuals with access to a public resource (a common) act in their own interest and, in doing so, ultimately deplete the resource. It assumes a somewhat dispiriting stance about people. The reality is more complex. The unquestioned dominance of this perspective has led to a situation where in the UK, the majority of our beautiful countryside is out of bounds for most of the population. **92% of the countryside has no right to roam and 97% of rivers have no uncontested rights of navigation**. \n\nInterestingly, Elinor also found field based cases that supported the Tragedy of the Commons view. These were situations characterised by **short term thinking**, an **absence of communication** between groups, a **lack of trust** for each other and no reliable access to **external enforcement** e.g. legal redress.\n \n\n\nOlson's logic of collective action contains similar bleak expectations of human nature. It is worth noting that Hardin and Olson were themselves restricted by their gender, socioeconomic status and geography. i.e. white european men. Why is this of relevance? Ask any breastfeeding mother or anyone who has children: What is your expected return on this investment? Is there anything 'rational' about having a child? It wrecks women's careers, bodies and finances - apparently it costs £100,000 to rear a single human to adulthood. \n\nElinor Ostrom challenged the Tragedy of the Commons argument with solid field based evidence."}]}}